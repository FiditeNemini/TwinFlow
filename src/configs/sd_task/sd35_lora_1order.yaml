model:
  model_name: SD3_5
  aux_time_embed: false
  model_path: path/to/stable-diffusion-3.5-medium
  lora_rank: 64
  lora_alpha: 64
  lora_target_modules: timestep_embedder.linear_1,timestep_embedder.linear_2,attn.add_k_proj,attn.add_v_proj,attn.add_q_proj,attn.to_add_out,attn.to_k,attn.to_out.0,attn.to_q,attn.to_v

data:
  height: 512
  width: 512
  train_dirs:
    - path/to/data
    
# -- This config uses a fixed teacher -- 
# method:
#   method_type: TwinFlow_LoRA
#   transport_type: Linear
#   consistc_ratio: 1.0
#   enhanced_ratio: 4.0
#   ema_decay_rate: 1.0
#   enhanced_range: [0.0, 1.0]
#   time_dist_ctrl: [1.0, 1.0, 1.0]
#   estimate_order: 1
#   loss_func_type: {"type": "barron_reweighting", "alpha": 1.0}
#   dist_match_cof: 0.5

# -- This config has no teacher -- 
method:
  method_type: TwinFlow_LoRA
  transport_type: Linear
  consistc_ratio: 1.0
  enhanced_ratio: 0.8
  ema_decay_rate: 0.0 # should be zero when estimate_order=1
  enhanced_range: [0.0, 1.0]
  time_dist_ctrl: [1.0, 1.0, 1.0]
  estimate_order: 1
  loss_func_type: {"type": "barron_reweighting", "alpha": 1.0}
  dist_match_cof: 0.5

# few-step sampling
sample:
  ckpt: "700"
  cfg_scale: 0
  cfg_interval: [0.00, 0.00]
  sampling_steps: 2 # 1
  stochast_ratio: 1.0 # 0.8
  extrapol_ratio: 0.0
  sampling_order: 1
  time_dist_ctrl: [1.0, 1.0, 1.0]
  rfba_gap_steps: [0.001, 0.7]
  sampling_style: few

# # any-step sampling
# sample:
#   ckpt: "700"
#   cfg_scale: 0
#   cfg_interval: [0.00, 0.00]
#   sampling_steps: 4 # 8
#   stochast_ratio: 0.0
#   extrapol_ratio: 0.0
#   sampling_order: 1
#   time_dist_ctrl: [1.0, 1.0, 1.0]
#   rfba_gap_steps: [0.001, 0.5]
#   sampling_style: any

# # multi-step sampling
# sample:
#   ckpt: "700"
#   cfg_scale: 0
#   cfg_interval: [0.00, 0.00]
#   sampling_steps: 30
#   stochast_ratio: 0.0
#   extrapol_ratio: 0.0
#   sampling_order: 1
#   time_dist_ctrl: [1.17, 0.8, 1.1]
#   rfba_gap_steps: [0.001, 0.0]
#   sampling_style: mul

train:
  load_checkpoint_path: ""
  output_dir: ../outputs
  seed: 42
  grad_accumulation_steps: 1
  micro_batch_size: 8
  num_train_epochs: 4
  optimizer: adamw
  lr: 1.0e-4
  betas:
    - 0.9
    - 0.99
  weight_decay: 0.0
  max_grad_norm: 1.0