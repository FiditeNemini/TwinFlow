model:
  type: ./networks/openuni/openuni_l_internvl3_2b_sana_1_6b_512_hf.py
  path: path/to/openuni_l_internvl3_2b_sana_1_6b_512_hf_blip3o60k.pth # https://huggingface.co/wusize/openuni/blob/main/openuni_l_internvl3_2b_sana_1_6b_512_hf_blip3o60k.pth
  in_chans: 16
  lora_rank: 64
  lora_alpha: 64
  lora_target_modules: time_embed.linear,timestep_embedder.linear_1,timestep_embedder.linear_2,attn1.add_k_proj,attn1.add_v_proj,attn1.add_q_proj,attn1.to_add_out,attn1.to_k,attn1.to_out.0,attn1.to_q,attn1.to_v,attn2.add_k_proj,attn2.add_v_proj,attn2.add_q_proj,attn2.to_add_out,attn2.to_k,attn2.to_out.0,attn2.to_q,attn2.to_v

data:
  height: 512
  width: 512
  train_dirs:
    - path/to/data

# -- This config uses a fixed teacher -- 
# method:
#   method_type: TwinFlow_LoRA
#   transport_type: Linear
#   consistc_ratio: 1.0
#   enhanced_ratio: 4.0
#   ema_decay_rate: 1.0
#   enhanced_range: [0.0, 1.0]
#   time_dist_ctrl: [1.0, 1.0, 1.0]
#   estimate_order: 1
#   loss_func_type: {"type": "barron_reweighting", "alpha": 1.0}
#   dist_match_cof: 0.75

# -- This config has no teacher -- 
method:
  method_type: TwinFlow_LoRA
  transport_type: Linear
  consistc_ratio: 1.0
  enhanced_ratio: 0.8
  ema_decay_rate: 0.0 # should be zero when estimate_order=1
  enhanced_range: [0.0, 1.0]
  time_dist_ctrl: [1.0, 1.0, 1.0]
  estimate_order: 1
  loss_func_type: {"type": "barron_reweighting", "alpha": 1.0}
  dist_match_cof: 0.75

# few-step sampling
sample:
  ckpt: "700"
  cfg_scale: 0
  cfg_interval: [0.00, 0.00]
  sampling_steps: 2 # 1
  stochast_ratio: 1.0 # 0.8
  extrapol_ratio: 0.0
  sampling_order: 1
  time_dist_ctrl: [1.0, 1.0, 1.0]
  rfba_gap_steps: [0.001, 0.7]
  sampling_style: few

# # any-step sampling
# sample:
#   ckpt: "700"
#   cfg_scale: 0
#   cfg_interval: [0.00, 0.00]
#   sampling_steps: 4 # 8
#   stochast_ratio: 0.0
#   extrapol_ratio: 0.0
#   sampling_order: 1
#   time_dist_ctrl: [1.0, 1.0, 1.0]
#   rfba_gap_steps: [0.001, 0.5]
#   sampling_style: any

# # multi-step sampling
# sample:
#   ckpt: "700"
#   cfg_scale: 0
#   cfg_interval: [0.00, 0.00]
#   sampling_steps: 30
#   stochast_ratio: 0.0
#   extrapol_ratio: 0.0
#   sampling_order: 1
#   time_dist_ctrl: [1.17, 0.8, 1.1]
#   rfba_gap_steps: [0.001, 0.0]
#   sampling_style: mul

train:
  load_checkpoint_path: ""
  output_dir: ../outputs
  seed: 42
  grad_accumulation_steps: 1
  micro_batch_size: 16
  num_train_epochs: 4
  optimizer: adamw
  lr: 1.0e-4
  betas:
    - 0.9
    - 0.95
  weight_decay: 0.0
  max_grad_norm: 1.0